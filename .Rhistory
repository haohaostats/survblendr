#   本脚本通过“三管齐下”的策略，确保生成平滑、富有层次感的“阶梯”式结果：
#     1. 低样本量 (n=100)      --> 增加任务难度
#     2. 精细的beta序列       --> 提高关键区域的“分辨率”
#     3. 高重复次数 (n=500)    --> 消除随机噪音，确保曲线平滑
#
# =============================================================================
# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# --- 阶段一：环境设置与依赖加载 ---
# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
rm(list = ls())
packages <- c("here", "survival", "Rcpp", "riskRegression", "data.table", "pracma", "knitr", "kableExtra", "pec", "penalized", "ggplot2", "tidyr")
for (pkg in packages) {
if (!require(pkg, character.only = TRUE)) {
install.packages(pkg)
library(pkg, character.only = TRUE)
}
}
cat("===== 阶段一完成：环境已设置，所有包已加载 =====\n\n")
# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# --- 阶段二：加载所有函数和 C++ 代码 ---
# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
original_wd <- here::here()
path_to_lu_2025 <- here::here("lu_2025")
if (!dir.exists(path_to_lu_2025)) { stop("错误：找不到 'lu_2025' 文件夹。") }
# ============================================================
# A General Approach for Sample Size with NPH + Cure
# Batch runner for ALL scenarios in Tables 1–5
# Requires: nphPower (>= 1.1.0), survival, data.table
# ============================================================
# ---- setup ----
SEED <- 20250809
set.seed(SEED)
need <- c("nphPower","survival","data.table")
to_install <- setdiff(need, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, repos = "https://cloud.r-project.org")
invisible(lapply(need, library, character.only = TRUE))
# ---- global design (from Section 4.1) ----
ENTRY <- 24   # months
FUP   <- 24   # months
ALPHA <- 0.05
BETA  <- 0.20
RATIO <- 1    # 1:1
# dropout/loss to follow-up not used in Tables 1–5 (admin censoring only)
DROP  <- c(0, 0)   # monthly dropout hazards before/after tfup
# Weight sets
W_LR   <- nphPower::gen.wgt("LR")
W_FH10 <- nphPower::gen.wgt("FH", c(1,0))
W_FH01 <- nphPower::gen.wgt("FH", c(0,1))
W_MAXC <- nphPower::gen.wgt("Maxcombo")
# Helpers to round total N to correct allocation block
.roundN <- function(N, ratio = RATIO){
# return nearest integer satisfying arm-size integrality
if (ratio == 1) return(ceiling(N/2)*2)
# generic: N0:N1 = 1 : ratio -> block size = (1+ratio)
blk <- 1 + ratio
ceiling(N / blk) * blk
}
# Extract events (if available) safely
.get_events <- function(fit){
nms <- names(fit)
cand <- c("events", "event", "totalEvents", "d")
k <- cand[cand %in% nms]
if (length(k)) return(as.numeric(fit[[k[1]]]))
return(NA_real_)
}
# ---- Control distributions for susceptible population ----
# Exponential: median=12 -> lambda = log(2)/12
get_ctrl_params_exp <- function() list(k0 = 1, lmd0 = log(2)/12)
# Weibull (susceptible): S(t) = exp(-lambda * t^k), ensure median=12
# k = 1.2 per paper; lambda = log(2)/12^k (consistent with S(12)=0.5)
get_ctrl_params_weib <- function(k = 1.2, med = 12) {
list(k0 = k, lmd0 = log(2) / (med^k))
}
# ---- cureHR wrappers (build overall hazard for control & HR(t) for framework) ----
# PHCM: theta applies to susceptible hazards
build_PHCM <- function(pi0, pi1, dist = c("exp","weibull"), theta_const){
dist <- match.arg(dist)
if (dist == "exp") {
p <- get_ctrl_params_exp()
} else {
p <- get_ctrl_params_weib(k = 1.2, med = 12)
}
theta_fun <- function(t) rep(theta_const, length(t))
nphPower::cureHR(pi0, pi1, p$k0, p$lmd0, theta_fun, "susceptible")
}
# PWPHCR (piecewise on susceptible): HR=1 before t0, HR=theta_after after t0
build_PWPHCR <- function(pi0, pi1, t0, theta_after = 0.7){
p <- get_ctrl_params_exp()  # Tables 2 use exponential for control
theta_fun <- function(t) ifelse(t <= t0, 1, theta_after)
nphPower::cureHR(pi0, pi1, p$k0, p$lmd0, theta_fun, "susceptible")
}
# NPHCRM (overall): HR applies to overall hazards (piecewise/dynamic)
build_NPHCRM_delay <- function(pi0, t0, theta_after = 0.7){
p <- get_ctrl_params_exp()
theta_fun <- function(t) ifelse(t <= t0, 1, theta_after)
# pi1 is ignored under "overall" internally; keep for signature
nphPower::cureHR(pi0, pi1 = pi0, p$k0, p$lmd0, theta_fun, "overall")
}
# NPHCRM others: diminishing / crossing
build_NPHCRM_other <- function(pi0, type = c("diminishing","crossing")){
type <- match.arg(type)
p <- get_ctrl_params_exp()
if (type == "diminishing") {
# theta(t) = (t/240 + 0.6) for t<48; =0.8 for t>=48 (months)
theta_fun <- function(t){
z <- ifelse(t < 48, (t/240) + 0.6, 0.8)
as.numeric(z)
}
} else {
# crossing: 1.1 for t<6, 0.65 for t>=6
theta_fun <- function(t) ifelse(t < 6, 1.1, 0.65)
}
nphPower::cureHR(pi0, pi1 = pi0, p$k0, p$lmd0, theta_fun, "overall")
}
# Three-component cure model (Kim & Gray). Return (hazR_overall, CtrlHaz_overall)
#   trt: pi0* (cured), pi1* (non-cured group 1 weight), lambdas for G1 and G2
#   ctrl: pi0 (cure), lambda0 (susceptible exp)
threeComp_chr <- function(pi0_ctrl, pi1_trt_overall, med_g1 = 24, med_g2 = 12){
# For experimental: cured pi*0 = pi1_trt_overall;
# Among susceptibles (1 - pi1), 60% from G1(med=med_g1), 40% from G2(med=med_g2)
pi_star0 <- pi1_trt_overall
w1 <- 0.6 * (1 - pi_star0)
# NOTE: Kim & Gray uses (pi*0, pi*1) with third group weight = 1 - pi*0 - pi*1
pi_star1 <- w1
lam1 <- log(2)/med_g1
lam2 <- log(2)/med_g2
s1 <- function(t) { pi_star0 + pi_star1*exp(-lam1*t) + (1 - pi_star0 - pi_star1)*exp(-lam2*t) }
f1 <- function(t) { pi_star1*lam1*exp(-lam1*t) + (1 - pi_star0 - pi_star1)*lam2*exp(-lam2*t) }
lam0 <- log(2)/12
s0 <- function(t) { pi0_ctrl + (1 - pi0_ctrl)*exp(-lam0*t) }
f0 <- function(t) { (1 - pi0_ctrl)*lam0*exp(-lam0*t) }
h0 <- function(t) f0(t)/s0(t)
hr <- function(t) (f1(t)/s1(t)) / h0(t)
list(hr = hr, h0 = h0)
}
# Generic wrapper to call pwr2n.NPH once
run_one <- function(CtrlHaz, hazR, W, ratio = RATIO){
fit <- nphPower::pwr2n.NPH(
method = "MaxLR",
entry = ENTRY, fup = FUP,
Wlist = W,
CtrlHaz = CtrlHaz, hazR = hazR,
transP1 = DROP, transP0 = DROP,
beta = BETA, alpha = ALPHA,
alternative = "two.sided",
ratio = ratio
)
N  <- .roundN(fit$totalN, ratio = ratio)
ev <- .get_events(fit)
list(N = N, events = ev)
}
# Run four tests and collate
run_all_tests <- function(CtrlHaz, hazR, ratio = RATIO){
out <- list(
LRT   = run_one(CtrlHaz, hazR, W_LR,   ratio),
FH10  = run_one(CtrlHaz, hazR, W_FH10, ratio),
FH01  = run_one(CtrlHaz, hazR, W_FH01, ratio),
MaxC  = run_one(CtrlHaz, hazR, W_MAXC, ratio)
)
data.table::data.table(
Test = names(out),
Events = sapply(out, \(z) z$events),
SampleSize = sapply(out, \(z) z$N)
)
}
# ============================================================
# Table 1: PHCM under Exponential & Weibull susceptible hazards
# Scenarios from Table 1 (pairs of (pi0, pi1) with theta)
# ============================================================
tab1_grid <- data.table::data.table(
pi0 = c(0,0, 0.15,0.15, 0.15,0.15,0.15, 0.15,0.15,0.15, 0.4,0.4, 0.4,0.4,0.4, 0.4,0.4,0.4),
pi1 = c(0,0, 0.15,0.15, 0.25,0.25,0.25, 0.4,0.4,0.4, 0.4,0.4, 0.45,0.45,0.45, 0.55,0.55,0.55),
theta = c(0.8,0.6, 0.8,0.6, 1.0,0.8,0.6, 1.0,0.8,0.6, 0.8,0.6, 1.0,0.8,0.6, 1.0,0.8,0.6)
)
run_table1 <- function(){
rows <- list()
for (i in seq_len(nrow(tab1_grid))){
r <- tab1_grid[i]
# Exponential
chE <- build_PHCM(r$pi0, r$pi1, "exp", theta_const = r$theta)
resE <- run_all_tests(CtrlHaz = chE[[1]], hazR = chE[[2]])
resE[, Dist := "E"][, pi0 := r$pi0][, pi1 := r$pi1][, theta := r$theta]
# Weibull
chW <- build_PHCM(r$pi0, r$pi1, "weibull", theta_const = r$theta)
resW <- run_all_tests(CtrlHaz = chW[[1]], hazR = chW[[2]])
resW[, Dist := "W"][, pi0 := r$pi0][, pi1 := r$pi1][, theta := r$theta]
rows[[i]] <- rbind(resE, resW)
}
out <- data.table::rbindlist(rows)
# Reshape to wide: one row per scenario with E/W pairs per test
out_w <- data.table::dcast(
out, pi0 + pi1 + theta + Test ~ Dist,
value.var = c("Events","SampleSize")
)
data.table::setcolorder(out_w, c("pi0","pi1","theta","Test",
"Events_E","SampleSize_E","Events_W","SampleSize_W"))
out_w[]
}
# ============================================================
# Table 2: PWPHCR (delayed effects on susceptible), HR=0.7 post t0
# t0 in {6, 9}; control = exponential
# Pairs (pi0, pi1) same as Table 1 subset
# ============================================================
tab2_pairs <- data.table::data.table(
pi0 = c(0, 0.15,0.15,0.15, 0.4,0.4,0.4),
pi1 = c(0, 0.15,0.25,0.4,  0.4,0.45,0.55)
)
t0_vec <- c(6, 9)
run_table2 <- function(){
rows <- list(); idx <- 1
for (j in t0_vec){
for (i in seq_len(nrow(tab2_pairs))){
r <- tab2_pairs[i]
ch <- build_PWPHCR(r$pi0, r$pi1, t0 = j, theta_after = 0.7)
res <- run_all_tests(CtrlHaz = ch[[1]], hazR = ch[[2]])
res[, `:=`(pi0 = r$pi0, pi1 = r$pi1, t0 = j)]
rows[[idx]] <- res; idx <- idx + 1
}
}
out <- data.table::rbindlist(rows)
data.table::dcast(out, pi0 + pi1 + t0 ~ Test, value.var = c("Events","SampleSize"))
}
# ============================================================
# Table 3: NPHCRM (delayed on overall), HR=0.7 post t0
# pi0 in {0.1,0.15,0.2,0.25,0.3}; t0 in {6,9}; control = exponential
# ============================================================
tab3_pi0 <- c(0.1,0.15,0.2,0.25,0.3)
run_table3 <- function(){
rows <- list(); idx <- 1
for (j in t0_vec){
for (p0 in tab3_pi0){
ch <- build_NPHCRM_delay(pi0 = p0, t0 = j, theta_after = 0.7)
res <- run_all_tests(CtrlHaz = ch[[1]], hazR = ch[[2]])
res[, `:=`(pi0 = p0, t0 = j)]
rows[[idx]] <- res; idx <- idx + 1
}
}
out <- data.table::rbindlist(rows)
data.table::dcast(out, pi0 + t0 ~ Test, value.var = c("Events","SampleSize"))
}
# ============================================================
# Table 4: NPHCRM (other NPH): diminishing & crossing
# pi0 in {0.1,0.15,0.2,0.25,0.3}; control = exponential
# ============================================================
run_table4 <- function(){
rows <- list(); idx <- 1
for (tp in c("diminishing","crossing")){
for (p0 in tab3_pi0){
ch <- build_NPHCRM_other(pi0 = p0, type = tp)
res <- run_all_tests(CtrlHaz = ch[[1]], hazR = ch[[2]])
res[, `:=`(pi0 = p0, Type = tp)]
rows[[idx]] <- res; idx <- idx + 1
}
}
out <- data.table::rbindlist(rows)
data.table::dcast(out, pi0 + Type ~ Test, value.var = c("Events","SampleSize"))
}
# ============================================================
# Table 5: Three-component cure model (Kim & Gray)
# (pi0, pi1) in {(0.1,0.3),(0.15,0.3),(0.2,0.4),(0.25,0.4),(0.3,0.5)};
# med_g1 = 24 months; med_g2 in {12, 8}; control susceptible med=12
# ============================================================
tab5_pairs <- data.table::data.table(
pi0 = c(0.1,0.1, 0.15,0.15, 0.2,0.2, 0.25,0.25, 0.3,0.3),
pi1 = c(0.3,0.3,  0.3,0.3,   0.4,0.4, 0.4,0.4,  0.5,0.5),
M3  = c(12, 8,   12, 8,     12, 8,   12, 8,    12, 8)
)
run_table5 <- function(){
rows <- list()
for (i in seq_len(nrow(tab5_pairs))){
r <- tab5_pairs[i]
eg <- threeComp_chr(pi0_ctrl = r$pi0,
pi1_trt_overall = r$pi1,
med_g1 = 24, med_g2 = r$M3)
res <- run_all_tests(CtrlHaz = eg$h0, hazR = eg$hr)
res[, `:=`(pi0 = r$pi0, pi1 = r$pi1, M3 = r$M3)]
rows[[i]] <- res
}
out <- data.table::rbindlist(rows)
data.table::dcast(out, pi0 + pi1 + M3 ~ Test, value.var = c("Events","SampleSize"))
}
# ============================================================
# Optional: empirical power check via simulation (LRT only)
# (Monte Carlo engine per Appendix A; uses overall survival So(t))
# ============================================================
# Build overall survival S_o(t) from (CtrlHaz, hazR). Numerical integration on grid.
make_So_funcs <- function(CtrlHaz, hazR, pi0, pi1 = NULL, type = c("overall","derived")){
# type="overall": So1 via lambda_o1(t) = hazR(t) * lambda_o0(t)
# type="derived": reserved (not used)
type <- match.arg(type)
# grid in months
dt <- 1/100
tmax <- ENTRY + FUP
grid <- seq(0, tmax, by = dt)
# control overall hazard
h0 <- CtrlHaz(grid)
H0 <- cumsum(h0)*dt
S0 <- exp(-H0)
# experimental
h1 <- pmax(1e-12, hazR(grid)) * h0
H1 <- cumsum(h1)*dt
S1 <- exp(-H1)
list(
grid = grid, dt = dt,
So0 = approxfun(grid, S0, rule = 2),
So1 = approxfun(grid, S1, rule = 2)
)
}
# Simulate one trial and compute LRT p via survdiff
sim_lrt_once <- function(So_list, N_total, ratio = RATIO){
n1 <- round(N_total * ratio/(1+ratio))
n0 <- N_total - n1
# uniform entry
u0 <- runif(n0, 0, ENTRY); u1 <- runif(n1, 0, ENTRY)
draw_T <- function(So){
u <- runif(length(So), 0, 1)
# inverse by lookup
# cured: u <= So(Inf) -> but with admin censoring only, cured prob tends to pi (plateau)
# approximate by taking largest grid point where S(t) > u
# Build vectorized inverse using approx monotone grid
# (Note: fine for our resolution; exact inverse not required to verify 80% target)
}
# We approximate event time by inverting survival via grid search
inv_S <- function(SoF, u){
# find earliest t with So(t) <= u
# use binary search over grid
t <- numeric(length(u))
G <- So_list$grid
Svals <- SoF(G)
for (i in seq_along(u)){
k <- which(Svals <= u[i])[1]
t[i] <- if (is.na(k)) max(G) else G[k]
}
t
}
# Arm 0
u0surv <- runif(n0); t0 <- inv_S(So_list$So0, u0surv)
# Arm 1
u1surv <- runif(n1); t1 <- inv_S(So_list$So1, u1surv)
# admin censoring at ENTRY+FUP for each subject
x0 <- pmin(t0, ENTRY + FUP - u0); d0 <- as.integer(t0 <= (ENTRY + FUP - u0))
x1 <- pmin(t1, ENTRY + FUP - u1); d1 <- as.integer(t1 <= (ENTRY + FUP - u1))
df <- data.frame(
time = c(x0, x1),
status = c(d0, d1),
arm = factor(c(rep(0, n0), rep(1, n1)))
)
# survdiff returns chisq ~ (LR)^2; 2-sided p-val
ss <- survival::survdiff(Surv(time, status) ~ arm, data = df, rho = 0)
pchisq(ss$chisq, df = 1, lower.tail = FALSE)
}
emp_power_LRT <- function(CtrlHaz, hazR, N, reps = 2000, ratio = RATIO){
So <- make_So_funcs(CtrlHaz, hazR, pi0 = NA)
ps <- replicate(reps, sim_lrt_once(So, N_total = N, ratio = ratio))
mean(ps < ALPHA)
}
# ============================================================
# Run all and export CSVs
# ============================================================
message("Running Table 1 ...")
tb1 <- run_table1()
data.table::fwrite(tb1, "Table1_PHCM.csv")
message("Saved -> Table1_PHCM.csv")
message("Running Table 2 ...")
tb2 <- run_table2()
data.table::fwrite(tb2, "Table2_PWPHCR_delay.csv")
message("Saved -> Table2_PWPHCR_delay.csv")
message("Running Table 3 ...")
tb3 <- run_table3()
data.table::fwrite(tb3, "Table3_NPHCRM_delay.csv")
message("Saved -> Table3_NPHCRM_delay.csv")
message("Running Table 4 ...")
tb4 <- run_table4()
data.table::fwrite(tb4, "Table4_NPHCRM_other.csv")
message("Saved -> Table4_NPHCRM_other.csv")
message("Running Table 5 ...")
tb5 <- run_table5()
data.table::fwrite(tb5, "Table5_ThreeComponent.csv")
message("Saved -> Table5_ThreeComponent.csv")
# Optional: quick LRT power spot-check for a couple scenarios (off by default)
DO_POWER_CHECK <- FALSE
if (DO_POWER_CHECK){
set.seed(SEED)
# Example check: Table 2, (pi0,pi1)=(0.15,0.25), t0=6, MaxCombo sample size (from our calc)
ch_ex <- build_PWPHCR(0.15, 0.25, t0 = 6, theta_after = 0.7)
N_LR <- tb2[pi0==0.15 & pi1==0.25 & t0==6, SampleSize_LRT]
pw <- emp_power_LRT(ch_ex[[1]], ch_ex[[2]], N = N_LR, reps = 500) # 500 quick reps
cat("\nQuick LRT power check (500 reps):", round(100*pw,1), "%\n")
}
message("Done.")
# ============================================================
setwd("E:/PHD/Project/2024/ASSB-XT2024/Draft/Submission/Rapp/aswb")
devtools::document()
devtools::document()
devtools::document()
devtools::document(); devtools::load_all()
devtools::document()
# data-raw/survblendr_demo_demo_weibull.R
# Generates an example Weibull dataset and stores it in the package.
# Re-run this script anytime you want to refresh the example.
stopifnot(requireNamespace("usethis", quietly = TRUE))
# Use the package's generator so it's consistent with the paper
if (!requireNamespace("survblend", quietly = TRUE)) stop("Run inside the aswb package.")
# data-raw/survblendr_demo_demo_weibull.R
# Generates an example Weibull dataset and stores it in the package.
# Re-run this script anytime you want to refresh the example.
stopifnot(requireNamespace("usethis", quietly = TRUE))
# Use the package's generator so it's consistent with the paper
if (!requireNamespace("survblend", quietly = TRUE)) stop("Run inside the aswb package.")
# data-raw/survblendr_demo_demo_weibull.R
# Generates an example Weibull dataset and stores it in the package.
# Re-run this script anytime you want to refresh the example.
stopifnot(requireNamespace("usethis", quietly = TRUE))
# Use the package's generator so it's consistent with the paper
if (!requireNamespace("survblend", quietly = TRUE)) stop("Run inside the aswb package.")
source("data-raw/survblendr_demo_weibull.R")
devtools::document()
devtools::document()
Select-String -Path R\*.R,man\*.Rd,vignettes\* -Pattern 'aswb|Run inside the aswb package' -List
find_leftovers <- function(pattern = "aswb|Run inside the aswb package",
paths = c("R","man","vignettes")) {
files <- unlist(lapply(paths, function(p) {
if (!dir.exists(p)) return(character(0))
list.files(p, recursive = TRUE, full.names = TRUE)
}))
out <- lapply(files, function(f) {
txt <- tryCatch(readLines(f, warn = FALSE, encoding = "UTF-8"), error = function(e) return(NULL))
if (is.null(txt)) return(NULL)
hit <- grep(pattern, txt)
if (!length(hit)) return(NULL)
data.frame(file = f, line = hit, text = trimws(substr(txt[hit], 1, 160)), stringsAsFactors = FALSE)
})
do.call(rbind, out)
}
hits <- find_leftovers()
hits
devtools::document()
find_leftovers <- function(pattern = "aswb|Run inside the aswb package",
paths = c("R","man","vignettes")) {
files <- unlist(lapply(paths, function(p) {
if (!dir.exists(p)) return(character(0))
list.files(p, recursive = TRUE, full.names = TRUE)
}))
out <- lapply(files, function(f) {
txt <- tryCatch(readLines(f, warn = FALSE, encoding = "UTF-8"), error = function(e) return(NULL))
if (is.null(txt)) return(NULL)
hit <- grep(pattern, txt)
if (!length(hit)) return(NULL)
data.frame(file = f, line = hit, text = trimws(substr(txt[hit], 1, 160)), stringsAsFactors = FALSE)
})
do.call(rbind, out)
}
find_leftovers()
# 1. 设置你的代码文件夹路径
folder_path <- "E:/PHD/Project/2024/ASSB-XT2024/Draft/Submission/Rapp/aswb/R"
# 2. 设置输出的txt文件路径和名称
output_file <- file.path(folder_path, "combined_R_scripts.txt")
# 3. 获取文件夹下所有的.R文件列表
r_files <- list.files(path = folder_path, pattern = "\\.R$", full.names = TRUE)
# 4. 循环读取每个R文件并写入到输出文件中
# file.create(output_file) # 创建一个空文件（如果需要）
# 使用lapply和cat，代码更简洁
all_content <- lapply(r_files, function(file) {
# 读取文件内容
content <- readLines(file, warn = FALSE)
# 在每个文件内容前添加一个分隔符和文件名，方便区分
c(
paste("\n\n#", "=================================================="),
paste("# START OF FILE:", basename(file)),
paste("#", "==================================================\n"),
content
)
})
# 将所有内容写入到目标文件中
writeLines(unlist(all_content), output_file)
# 打印成功信息
print(paste("所有R脚本已成功汇总到:", output_file))
survblendr_demo <- demo_sim_weibull(n = 1500, shape = 1.6, S20 = 0.20,
censor_rate = 0.40, t_max = 10, seed = 1)
load("data/aswb_demo.rda")
survblendr_demo <- aswb_demo
usethis::use_data(survblendr_demo, overwrite = TRUE, compress = "xz")
data(survblendr_demo)
devtools::document()
devtools::load_all()
library(survblendr)
data(survblendr_demo)
str(survblendr_demo)
# 最小自测
fit <- survblendr_extrapolate(survblendr_demo, t_obs=10, t_max=25, interval=1,
anchor_t=25, anchor_mean_Sa=0.035,
nsim_inla=200, nsim_ext=200, inla_threads=1, seed=20240901)
fit2 <- survblendr_extrapolate(survblendr_demo, t_obs=10, t_max=25, interval=1,
anchor_t=25, anchor_mean_Sa=0.035,
nsim_inla=200, nsim_ext=200, inla_threads=1, seed=20240901)
all.equal(fit1$S_blend, fit2$S_blend)  # TRUE
fit1 <- survblendr_extrapolate(survblendr_demo, t_obs=10, t_max=25, interval=1,
anchor_t=25, anchor_mean_Sa=0.035,
nsim_inla=200, nsim_ext=200, inla_threads=1, seed=20240901)
fit2 <- survblendr_extrapolate(survblendr_demo, t_obs=10, t_max=25, interval=1,
anchor_t=25, anchor_mean_Sa=0.035,
nsim_inla=200, nsim_ext=200, inla_threads=1, seed=20240901)
all.equal(fit1$S_blend, fit2$S_blend)  # TRUE
all.equal(fit1$S_obs,   fit2$S_obs)    # TRUE
all.equal(fit1$S_ext,   fit2$S_ext)    # TRUE
p <- plot_curves(fit1); print(p)
library(ggplot2)
p <- plot_curves(fit1)
ggsave("man/figures/example_plot.svg", plot = p, width = 8, height = 6)
survblendr_summary_table(fit1, years = 10:25)
summary_df <- survblendr_summary_table(fit2, years = 10:25)
knitr::kable(summary_df, digits = 3, format = "pipe")
